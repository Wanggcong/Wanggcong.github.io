
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Guangcong Wang; 王广聪; Computer Vision; Deep Learning; Sun Yat-set University, Nanyang Technological University; NTU">
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.min.js"></script>
<link rel="author" href="https://wanggcong.github.io/">

    <title>Guangcong Wang - Homepage</title>
    <style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : rgb(224, 224, 224); }
.title { width : 650px; margin : 20px auto; }
.container { width : 750px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
.container_title { width : 750px; margin : 20px auto; border-radius: 10px;  padding : 20px;  clear:both;}
.iframe_video {float: left; margin-right: 30px}
#bio {
    padding-top : 0px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; margin-right : 100px; border : 0 solid black; float : left; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
/* .publogo { width: 100 px; margin-right : 20px; float : left; border : 0;} */
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publogo2 { width: 100 px; margin-right : 20px; float : left; border : 0; padding-bottom : 10px;}
.publogo3 { width: 100 px; margin-right : 20px; float : left; border : 0; padding-bottom : 20px;}
.publogo4 { width: 100 px; margin-right : 20px; float : left; border : 0; padding-bottom : 30px;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
	<script async="" src="./homepage.js"></script>
</head>


<body>

    <div class="container">
        <h2>
            <a href="./index.html">Home</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./team.html">Team</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./publications.html">Publications</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
	        <a href="./awards.html">Awards</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./datasets.html">Datasets</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://github.com/wanggcong/">Github</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://scholar.google.com/citations?user=dk8EnkoAAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://wanggrun.github.io/">Related Link</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./recruit2024.html">Join Us</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://Wanggcong.github.io/index_chinese">中文版本</a><br>
        </h2>
    </div>

	<div class="container">
		<div id="sidebar">
            <!-- <img style=”float: left; padding: 28px 0px 0px 0px;” src="./wanggcong.jpeg" vspace="10 px" width="180 px" id="me" itemprop="photo"> -->
            <!-- <img style=”float: left; padding: 50px 0px 50px 0px;” src="./wanggcong.jpeg" vspace="10 px" width="180 px" id="me" itemprop="photo"> -->
            <img style=”float: left; padding: 0px 0px 0px 0px;” src="./wanggcong.jpeg" width="180 px" id="me" itemprop="photo">
            <!-- <img style=”float: left; padding: 100px 0px 0px 0px;” src="./wanggcong.jpeg" vspace="0 px" width="180 px" id="me" itemprop="photo"> -->
        </div>
		<div id="bio">

            <!-- <br> -->

			<!-- <h2>
				<span itemprop="name">Guangcong Wang <font size="4">王广聪</font> </span>
			</h2> -->

            <!-- <span itemprop="name"><font size="4">Guangcong Wang</font> <font size="4">王广聪</font> </span> -->

            <!-- <br> -->

			<!-- <p style="line-height:23px;"> -->
            <p style="line-height:18px;">
                <b><font size="4">Guangcong Wang</font> <font size="4">王广聪</font></b>
                <br>
                <br>
				Assistant Professor
                <br>
                School of Computing and Information Technology
                <br>
                Great Bay University
                <br>
                Email: wanggc3 at gmail.com 
                <br>
                Address: 508B, No. 16, University Road, Songshan Lake, Songshanhu International Community, Dongguan, Guangdong, China  
                <br>
		    </p>

            <br>
		</div>
	</div>

	<div class="container">
		<h2>Short Bio</h2>

        <p>Guangcong Wang is an Assistant Professor of Computing and Information Technology at Great Bay University. From April 2021 to April 2024, he worked as a postdoctral research fellow in <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a> at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>, working with <a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a> and <a href="https://www.mmlab-ntu.com/person/ccloy/">Prof. Chen Change (Cavan) Loy</a>. In December 2020, he received a PhD degree in the <a href="https://cse.sysu.edu.cn/">School of Computer Science</a> at <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-set University</a>, supervised by <a href="https://cse.sysu.edu.cn/content/2498">Prof. Jianhuang Lai</a> and also co-supervised by <a href="https://cse.sysu.edu.cn/content/2478"> Prof. Xiaohua Xie</a>.</p>
        <p>He is leading Vision, Graphics, and X Group (<a href="./team.html">VGX</a>) at Great Bay Univeristy. </p>
        <p>His research interests include 3D vision and AI4Science. He has published some papers, such as TPAMI, TOG, TNNLS, TIP, TCSVT, ICCV, CVPR, ECCV, NeurIPS, KDD, and AAAI.  </p>
        <p>He served as an Area Chair of <a href="https://bmvc2024.org/">BMVC 2024</a>. He als served as a reviewer of TPAMI, IJCV, SIGGRAPH, ICML, NeurIPS, ICCV, CVPR, ECCV, ICLR, AAAI, ACM MM TIP, TCSVT, etc.</p>
        <p>He was awarded Excellent Doctoral Dissertation Award of <a href="http://www.cesexd.com/"> China Education Society of Electronics</a>. </p >
        <p>He was awarded <a href="https://x.com/ICCVConference/status/1707400996228378992?s=20">Outstanding Reviewer</a> of ICCV 2023. 	
        <p>He is also a member of <a href="https://thegreatailab.github.io/">The Great AI Lab</a>. <font color="red"> Looking for self-motivated postdocs, PhDs, Masters, RAs, visiting students, and interns. Please drop me an email if interested.</font> <a href="./recruit2024.html">Link</a>.</p>

	</div>
	
    <div class="container">    
        <h2>Teaching</h2>
        <h4> Generative Artificial Intelligence (Ziyue Qiao, Guangcong Wang)</h4>
        <h4> Data and Algorithm </h4>
    </div>    

	<div class="container">
        <h4>Research Topics</h4>
        <li>3D/4D Vision: 3D/4D Reconstruction, Generation, Editing, Driving  </li>
        <li>Video Generation and semantic understanding: Video Generation, Editing, including Talking Face、digital human etc. </li>
        <li>Vision Perception and large language model</li>
        <li>Embodied AI</li>
        <li>AI for Science</li>
    
        <h4>Research Goals</h4>
        <li>3D Virtual World Systems, such as Open 3D Virtual Reality (<a href="https://open3dvr.github.io/index.html">Open3DVR</a>)</li>
        <li>AI4Science Systems, such as Open Biotechnology and Machine Learning (<a href="https://openbiotechml.github.io/index.html">OpenBiotechML</a>)</li>
        
    </div>     


	<!-- <div class="container">
	<h2>Goals</h2>
		
	<p>He has two long-term goals in this life. One is to build a plausible 3D virtual world system with machine learning methods, called Open 3D Virtual Reality (<a href="https://open3dvr.github.io/index.html">Open3DVR</a>). The other is to build a biotechnologial system with machine learning methods to solve biotechnologial problems in the future, called Open Biotechnology and Machine Learning (<a href="https://openbiotechml.github.io/index.html">OpenBiotechML</a>). </p>
	<p>Open3DVR and OpenBiotechML are non-profit open research platforms, which focus on advancing technologies and benefit humanity.</p>
	<p>Life is short. He will spend 30 years to develop related technologies to achieve these two goals.</p>
	<p><em>“For every yes, there are a thousand no's”</em></p>
	<p><em>	“A PATH IN THE ORDINARY” </em></p>
	<p><a href="https://open3dvr.github.io/index.html"><img src="./homepage_files/open3dvr_logo.png" style="width:200px;height:50px;"></a> &nbsp&nbsp&nbsp <a href="https://openbiotechml.github.io/index.html"><img src="./homepage_files/openbiotechml_logo.png" style="width:300px;height:50px;"></a></p>
</body>
	</div> -->

    <div class="container">
    <h2>News</h2>
    <p><font color='black'>[2024-09]</font> <a href="https://arxiv.org/abs/2409.18938/">A survey on Long Video Understanding (Arxiv'24) </a> released.
    <p><font color='black'>[2024-09]</font> <a href="https://mp.weixin.qq.com/s/Thw0Soi6tcN2AlM5otfSZA">Invited talk</a> at <a href="https://vcc.tech/index">VCC Group</a> on 3D Scene Creation and Editing.
    <p><font color='black'>[2024-08]</font> <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid++ (TPAMI'24) </a> (proj page, code, and demo) is released. Welcome to use our code. Fast-Vid2Vid++ is a real-time video translation method. For example, we can traslate semantic segmentation videos to RGB videos.
    <p><font color='black'>[2024-07]</font> <a href="https://wildavatar.github.io/">WildAvatar (Arxiv'24)</a>  (proj page, code, and demo) is released. Welcome to use our dataset and toolbox. WildAvatar is a large-scale dataset from YouTube with 10,000+ human subjects, designed to address the limitations of existing laboratory datasets for avatar creation.
    <p><font color='black'>[2024-05]</font> <a href="https://mvsgaussian.github.io/">MVSGaussian (ECCV'24)</a> (proj page, code, and demo) is released. Welcome to use our code. MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes. It offers high-quality initialization for fast training and real-time rendering.
    <p><font color='black'>[2024-05]</font> Area Chair of the British Machine Vision Conference <a href="https://bmvc2024.org/">(BMVC) 2024</a>.
	<p><font color='black'>[2024-04]</font> I joined <a href="https://www.gbu.edu.cn/?lang=en">Great Bay University</a> as an Assistant Professor. <font color="black"> Looking for self-motivated postdocs, PhDs, Masters, RAs, visiting students, and interns. Please drop me an email if interested.</font> <a href="./recruit2024.html">Link</a>. <!--https://github.com/Wanggcong/Wanggcong.github.io/blob/master/recruit.md-->
    <p><font color='black'>[2024-04]</font> <a href="https://perf-project.github.io/">PERF (TPAMI'24)</a> is a 360-degree novel view synthesis method that trains a panoramic neural radiance field from a single panorama. PERF can be applied to many applications, such as Panorama-to-3D, Text-to-3D, and Instruct 3D Stylization. Welcome to use our code.    
    <p><font color='black'>[2023-12]</font> <a href="https://frozenburning.github.io/projects/primdiffusion/">PrimDiffusion (NeurIPS'23)</a> is released. Welcome to use our code for 3D human generation.	
	<p><font color='black'>[2023-09]</font> Very happy to be recognized as an <a href="https://x.com/ICCVConference/status/1707400996228378992?s=20">Outstanding Reviewer</a> of ICCV 2023. 
	<p><font color='black'>[2023-07]</font> <a href="https://sparsenerf.github.io/">SparseNeRF (ICCV'23)</a> is released. Code is released at this <a href="https://github.com/Wanggcong/SparseNeRF">link</a>. Welcome to use our code. SparseNeRF aims at 3D reconstruction, synthesizing novel views given a few images.   
	<p><font color='black'>[2023-04]</font> Program Chair of <a href="https://www.mmlab-ntu.com/cvpr_ntu/seminar_2023/index.html">Pre-CVPR@NTU</a>.</p>  
	<p><font color='black'>[2023-04]</font> Code of <a href="https://scene-dreamer.github.io/">SceneDreamer(TPAMI'23)</a> is released. Welcome to create your 3D world at <a href="https://huggingface.co/spaces/FrozenBurning/SceneDreamer">Huggingface</a>.</p> 
	<p><font color='black'>[2022-08]</font> <a href="https://frozenburning.github.io/projects/text2light/">Text2Light (TOG 2022 Proc. SIGGRAPH Asia'22, journal-track)</a> is released. Welcome to make your high-quality HDR panoramas with texts at <a href="https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb">Colab</a>. </p> <!-- <font color='red'>NEW!</font> -->
	<p><font color='black'>[2022-07]</font> <a href="https://style-light.github.io/">StyleLight (ECCV'22)</a> and <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid (ECCV'22)</a>. Welcome to use StyleLight to generate HDR panoramas given an LDR FOV image and use Fast-Vid2Vid to compress video-to-video GAN for translation.</p> <!-- <font color='red'>NEW!</font> -->
    <p><font color='black'>[2021-07]</font> <a href="https://github.com/wanggrun/triplet">Efficient Self-supervised Learning (ICCV'23, oral)</a> is released. Welcome to use our code.</p>
    <p><font color='black'>[2021-06]</font> <a href="https://ieeexplore.ieee.org/document/9470916">Unsupervised Cross-modality Learning (TIP'21)</a>, the first work for unsupervised cross-modality person re-identification</p>
    <p><font color='black'>[2021-04]</font> I joined <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>. </p>
    <p><font color='black'>[2021-03]</font> <a href="https://ieeexplore.ieee.org/document/9408408">Architecture Adaptation (TNNLS'21)</a> and <a href="https://arxiv.org/abs/2105.06714">Adaptive VSO (ICME, oral)</a>.</p>
    <p><font color='black'>[2020-12]</font> I have received my PhD degree (CS) from Sun Yat-set University (<a href="https://mp.weixin.qq.com/s/D4F52P-ZXTloxoEXDkAV-g">Excellent Doctoral Dissertation Award of China Education Society of Electronics</a> and <a href="http://cse.sysu.edu.cn/content/6175">Excellent Doctoral Dissertation of SYSU</a>).</p> 
    <p><font color='black'>[2020-12]</font> Media Coverage: A large-scale person re-identification dataset named <a href="https://github.com/wanggrun/SYSU-30k">SYSU-30k</a> is released, <a href="https://mp.weixin.qq.com/s/XsE0wBECzO6Mddtfdu8vVA">link</a>. The dataset contains lots of persons, which might be used for person-centric tasks. Welcome to use our dataset.<br></p>
    </div>


    <div class="container">    
    <h2>Academic Services</h2>
	
    <h4> Program Chair of <a href="https://www.mmlab-ntu.com/cvpr_ntu/seminar_2023/index.html">Pre-CVPR@NTU</a> </h4>
    <h4> Area Chair of the <a href="https://bmvc2024.org/">British Machine Vision Conference (BMVC) 2024</a> </h4>

    <h4>Conferences (Reviewer)</h4>
    <li>SIGGRAPH </li>
    <li>SIGGRAPH Asia </li>
    <li>International Conference on Learning Representations (ICLR) </li>
    <li>International Conference on Machine Learning (ICML) </li>
    <li>Neural Information Processing Systems (NeurIPS)</li>    
    <li>International Conference on Computer Vision (ICCV) (<a href="https://x.com/ICCVConference/status/1707400996228378992?s=20">Outstanding Reviewer</a>) </li>
    <li>Computer Vision and Pattern Recognition (CVPR) </li>
    <li>European Conference on Computer Vision (ECCV) </li>
    <li>Association for the Advancement of Artificial Intelligence (AAAI) </li>
    <li>Others: ACM MM, ICPR, ACCV </li>
	    
    <h4>Journals (Reviewer)</h4>
    <li>IEEE Transactions on Pattern Analysis and Machine Intelligence （TPAMI） </li>
    <li>International Journal of Computer Vision (IJCV) </li>
    <li>IEEE Transactions on Visualization and Computer Graphics (TVCG)</li>
    <li>IEEE Transactions on Image Processing (TIP) </li>
    <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) </li>
    <li>Others: TOMM, Neurocomputing, SPL, etc
    <!-- <li>Others: TOMM, Neurocomputing,SPL, The Visual Computer, Image and Vison Computing -->
    <!-- <li>Others: TOMM, Neurocomputing,Signal Processing Letters, The Visual Computer, Image and Vison Computing -->
    </div>


    <div class="container">
	<h2>Selected Publications</h2> 
	<p>* denotes equal contribution</p>

    <!-- <img style=”float: left; padding: 28px 0px 0px 0px;” src="./wanggcong.jpeg" vspace="10 px" width="180 px" id="me" itemprop="photo"> -->

    
    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/long_video_understanding_logo.png" class="publogo4" width="200 px">
        <p> 
            <strong>
                <a href="https://arxiv.org/abs/2409.18938">From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding</a>
            </strong>
            <br>
            Heqing Zou, Tianze Luo, Guiyang Xie, Victor Zhang, Fengmao Lv, <b>Guangcong Wang</b>, Juanyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang
            <br>
            <em>Arxiv, 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2409.18938">PDF [Survey]</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>
    
    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/fast-vid2vid++_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid++: Spatial-Temporal Distillation for Real-Time Video-to-Video Synthesis</a>
            </strong>
            <br>
            Long Zhuo, <b>Guangcong Wang</b>, Shikai Li, Wayne Wu, Ziwei Liu.
            <br>
            <em> IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2207.05049">PDF</a>
                <a href="https://fast-vid2vid.github.io/">Project Page</a>
                <a href="https://github.com/fast-vid2vid/fast-vid2vid">Code</a>
                <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>
    


    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/wildavatar_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://wildavatar.github.io/">WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</a>
            </strong>
            <br>
            Zihao Huang, Shoukang Hu, <b>Guangcong Wang</b>, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.
            <br>
            <em>Arxiv, 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/pdf/2407.02165v2">PDF</a>
                <a href="https://wildavatar.github.io/">Project Page</a>
                <a href="https://github.com/wildavatar/WildAvatar_Toolbox">Code</a> 
                <a href="https://www.youtube.com/watch?v=T-XafMVKY7E">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>


        <div class="publication">
            <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/mvsgaussian_logo.gif" class="publogo2" width="200 px">
            <p> 
                <strong>
                    <a href="https://mvsgaussian.github.io/">MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo</a>
                </strong>
                <br>
                Tianqi Liu, <b>Guangcong Wang</b>, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2024</em>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2405.12218">PDF</a>
                    <a href="https://mvsgaussian.github.io/">Project Page</a>
                    <a href="https://github.com/TQTQliu/MVSGaussian">Code</a>
                    <a href="https://m.youtube.com/watch?v=yzYVY7apyJE&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>


        <div class="publication">
            <img src="./homepage_files/perf_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://perf-project.github.io/">PERF: Panoramic Neural Radiance Field from a Single Panorama</a>
                </strong>
		<br>
		<b>Guangcong Wang</b>*, Peng Wang*, Zhaoxi Chen, Wenping Wang, Chen Change Loy, Ziwei Liu
		<br>
                <em> IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 2024</em>
		<br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2310.16831">PDF</a>
                    <a href="https://perf-project.github.io/">Project Page</a>
                    <a href="https://github.com/perf-project/PeRF">Code</a>
                    <a href="https://www.youtube.com/watch?v=4wa2h1fjh2U&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
        <div class="publication">
            <img src="./homepage_files/primdiffusion_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://frozenburning.github.io/projects/primdiffusion/">PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation</a>
                </strong>
		<br>
		Zhaoxi Chen, Fangzhou Hong, Haiyi Mei, <b>Guangcong Wang</b>, Lei Yang, Ziwei Liu
		<br>
                <em> Neural Information Processing Systems (NeurIPS), 2023</em>
		<br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2312.04559">PDF</a>
                    <a href="https://frozenburning.github.io/projects/primdiffusion/">Project Page</a>
                    <a href="https://github.com/FrozenBurning/PrimDiffusion">Code</a>
                    <a href="https://www.youtube.com/watch?v=zprHGZ7Gm7A&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>

	    
        <div class="publication">
            <img src="./homepage_files/sparsenerf_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://https://sparsenerf.github.io/">SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis</a>
                </strong>
                <br>
                <b>Guangcong Wang</b>, Zhaoxi Chen, Chen Change Loy, Ziwei Liu.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2023</em>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2303.16196">PDF</a>
                    <a href="https://sparsenerf.github.io/">Project Page</a>
                    <a href="https://github.com/Wanggcong/SparseNeRF">Code</a>
                    <a href="https://www.youtube.com/watch?v=V0yCTakA964&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
	    
        <div class="publication">
            <img src="./homepage_files/scenedreamer_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://scene-dreamer.github.io/">SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections</a>
                </strong>
                <br>
                Zhaoxi Chen, <b>Guangcong Wang</b>, Ziwei Liu.
                <br>
                <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</em>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2302.01330">PDF</a>
                    <a href="https://scene-dreamer.github.io/">Project Page</a>
                    <a href="https://github.com/Scene-Dreamer/SceneDreamer">Code</a>
                    <a href="https://www.youtube.com/watch?v=nEfSKL2_FoA">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
        <div class="publication">
            <img src="./homepage_files/text2light_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://frozenburning.github.io/projects/text2light/">Text2Light: Zero-Shot Text-Driven HDR Panorama Generation</a>
                </strong>
                <br>
                <br>
                Zhaoxi Chen, <b>Guangcong Wang</b>, Ziwei Liu.
                <br>
                <em>ACM Transactions on Graphics (SIGGRAPH Asia), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2209.09898">PDF</a>
                    <a href="https://frozenburning.github.io/projects/text2light/">Project Page</a>
                    <a href="https://github.com/FrozenBurning/Text2Light">Code</a>
                    <a href="https://www.youtube.com/watch?v=XDx6tOHigPE">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
	<br>
         <div class="publication">
            <img src="./homepage_files/stylelight_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://style-light.github.io/">StyleLight: HDR Panorama Generation for Lighting Estimation and Editing</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Yinuo Yang, Chen Change Loy, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2207.14811">PDF</a>
                    <a href="https://style-light.github.io/">Project Page</a>
                    <a href="https://github.com/Wanggcong/StyleLight">Code</a>
                    <a href="https://www.youtube.com/watch?v=sHeWK1MSPg4">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	         

        <div class="publication">
            <img src="./homepage_files/fastvid2vid_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis</a>
                </strong>
                <br>
                <br>
                Long Zhuo, <b>Guangcong Wang</b>, Shikai Li, Wayne Wu, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2207.05049">PDF</a>
                    <a href="https://fast-vid2vid.github.io/">Project Page</a>
                    <a href="https://github.com/fast-vid2vid/fast-vid2vid">Code</a>
                    <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 
        
        <div class="publication">
            <img src="./homepage_files/triplet_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/wanggrun/triplet">Solving Inefficiency of Self-supervised Representation Learning</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, Keze Wang, <b>Guangcong Wang</b>, Philip Torr, Liang Lin.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Solving_Inefficiency_of_Self-Supervised_Representation_Learning_ICCV_2021_paper.pdf">PDF</a>
                    <a href="https://github.com/wanggrun/triplet">Code</a>
                    <a href="https://arxiv.org/abs/2104.08760">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/h2h_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9470916">Homogeneous-to-Heterogeneous:  Unsupervised Learning for  RGB-Infrared Person Re-Identification</a>
                </strong>
                <br>
                <br>
                Wenqi Liang, <b>Guangcong Wang</b>, Jianhuang Lai, Xiaohua Xie.
                <br>
                <em>IEEE Transactions on Image Processing (TIP), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9470916">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/Joint Learning_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9408408">Joint Learning of Neural Transfer and Architecture Adaptation for Image Recognition</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, Liang Lin, Rongcong Chen, <b>Guangcong Wang</b>, Jiqi Zhang.
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9408408">PDF</a> 
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         

        <div class="publication">
            <img src="./homepage_files/Confidence_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://arxiv.org/abs/2105.06714">Confidence-Guided Adaptive Gate and Dual Differential Enhancement for Video Salient Object Detection</a>
                </strong>
                <br>
                <br>
                Peijia Chen, Jianhuang Lai, <b>Guangcong Wang</b>, Huajun Zhou.
                <br>
                <em>IEEE International Conference on Multimedia and Expo (ICME), 2021 (oral)</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2105.06714">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/Smoothing_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">Smoothing Adversarial Domain Attack and p-Memory Reconsolidation for Cross-Domain Person Re-Identification</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Wenqi Liang, Guangrun Wang.
                <br>
                <em>Computer Vision and Pattern Recognition Conference (CVPR) , 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         
    
        <div class="publication">
            <img src="./homepage_files/weaklyreid_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/wanggrun/SYSU-30k">Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, <b>Guangcong Wang</b>, Xujie Zhang, Jianhuang Lai, Liang Lin.
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/1904.03845">PDF</a>
                    <a href="https://github.com/wanggrun/SYSU-30k">Dataset, Code, Pretrained model</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 


        <div class="publication">
            <img src="./homepage_files/treeconv_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://www.kdd.org/kdd2020/accepted-papers/view/grammatically-recognizing-images-with-tree-convolution">Grammatically Recognizing Images with Tree Convolution</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, <b>Guangcong Wang</b>, Keze Wang, Xiaodan Liang, Liang Lin.
                <br>
                <em>ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://www.kdd.org/kdd2020/accepted-papers/view/grammatically-recognizing-images-with-tree-convolution">PDF</a>
                    <a href="https://github.com/wanggrun/TreeConv/stargazers">Code</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>   

        <div class="publication">
            <img src="./homepage_files/streid_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Spatial-Temporal Person Re-identification</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Peigen Huang and Xiaohua Xie.
                <br>
                <em>The Association for the Advancement of Artificial Intelligence (AAAI), 2019</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4921">PDF</a>
                    <a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Code</a>
                    <a href="https://arxiv.org/abs/1812.03282">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

      

        <div class="publication">
            <img src="./homepage_files/m2m_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://arxiv.org/pdf/1811.03768.pdf">M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification</a>
                </strong>
                <br>
                <br>
                Wenqi Liang, <b>Guangcong Wang</b>, Jianhuang Lai.
                <br>
                <em>ACTA AUTOMATICA SINICA, 2019</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/1811.03768.pdf">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 
        <div class="publication">
            <img src="./homepage_files/Ocluded_ReID_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://arxiv.org/pdf/1804.02792.pdf">Ocluded Person Re-identification</a>
                </strong>
                <br>
                <br>
                Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, <b>Guangcong Wang</b>.
                <br>
                <em>IEEE International Conference on Multimedia and Expo (ICME), 2021 (oral)</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/1804.02792.pdf">PDF</a>
		    <a href="https://github.com/tinajia2012/ICME2018_Occluded-Person-Reidentification_datasets">Dataset</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 	    
        <div class="publication">
            <img src="./homepage_files/P2SNet_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8025424">P2SNet: Can an Image Match a Video for Person Re-identification in an End-to-end Way</a>
                </strong>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Xiaohua Xie.
                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</em>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8025424">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         

        <div class="publication">
            <img src="./homepage_files/DGL_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf">Deep Growing Learning</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Xiaohua Xie, Jianhuang Lai, Jiaxuan Zhuo.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2017</em>
                <br>
                <br>
                <span class="links">
                    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf">PDF</a>
                    <a href="https://github.com/Wanggcong/Deep-growing-learning">Code</a>
                    
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>  

        <!-- //cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=DEKzN_4t5lNck7an-VfV9GZNqykDXeskr7_d56i0Qhw -->
        <!-- //www.clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=DEKzN_4t5lNck7an-VfV9GZNqykDXeskr7_d56i0Qhw -->
        <!-- //www.clustrmaps.com/map_v2.png?d=DEKzN_4t5lNck7an-VfV9GZNqykDXeskr7_d56i0Qhw&cl=ffffff -->
	<div style="text-align:center">
            <a href="https://clustrmaps.com/site/1bw2p"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=DEKzN_4t5lNck7an-VfV9GZNqykDXeskr7_d56i0Qhw" /></a>
        </div>
	    
        <div style="text-align:right">
            Website template credits to <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        </div>
	    
	<!-- <div style="text-align:right">
            <img src="https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2FWanggcong%2FWanggcong.github.io%2Fblob%2Fmaster%2Findex.html&label=stats&countColor=%23263759" /></a>
        </div>	     -->


	</div>	

       

</body></html>
