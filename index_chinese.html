
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Guangcong Wang; 王广聪; 计算机视觉; 深度学习; 大湾区大学， 中山大学; 南洋理工大学; GBU; SYSU; NTU">
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.min.js"></script>
<link rel="author" href="https://wanggcong.github.io/">

    <title>Guangcong Wang - Homepage</title>
    <style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : rgb(224, 224, 224); }
.title { width : 650px; margin : 20px auto; }
.container { width : 750px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
.container_title { width : 750px; margin : 20px auto; border-radius: 10px;  padding : 20px;  clear:both;}
.iframe_video {float: left; margin-right: 30px}
#bio {
    /* padding-top : 20px; */
    padding-top : 0px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; margin-right : 100px; border : 0 solid black; float : left; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publogo2 { width: 100 px; margin-right : 20px; float : left; border : 0; padding-bottom : 10px;}
.publogo3 { width: 100 px; margin-right : 20px; float : left; border : 0; padding-bottom : 20px;}
.publogo4 { width: 100 px; margin-right : 20px; float : left; border : 0; padding-bottom : 35px;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
	<script async="" src="./homepage.js"></script>
</head>


<body>

    <div class="container">
        <h2>
            <a href="./index_chinese.html">主页</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./team.html">团队</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./publications.html">出版</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
	        <a href="./awards.html">奖励荣誉</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./datasets.html">数据集</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://github.com/wanggcong/">Github代码</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://scholar.google.com/citations?user=dk8EnkoAAAAJ&hl=en">谷歌学术</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://wanggrun.github.io/">相关链接</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./recruit2024.html">加入我们</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://Wanggcong.github.io/index">English Version</a><br>
        </h2>
    </div>

	<div class="container">
		<div id="sidebar">
            <!-- <img src="./wanggcong.jpeg" vspace="10 px" width="180 px" id="me" itemprop="photo"> -->
            <img style=”float: left; padding: 0px 0px 0px 0px;” src="./wanggcong.jpeg" width="180 px" id="me" itemprop="photo">
        </div>
		<div id="bio">

            <!-- <br> -->

			<!-- <h2>
				<span itemprop="name">王广聪 <font size="4">  Guangcong Wang</font> </span>
			</h2> -->

            <!-- <br> -->

			<p style="line-height:23px;">
                <b><font size="4">王广聪</font> <font size="4"> Guangcong Wang</font></b>
                <br>
                <br>
				助理教授
                <br>
                大湾区大学, 计算机信息科学技术学院                
                <br>
                邮箱: wanggc3 at gmail.com 
                <br>
                地址: 中国广东省东莞市松山湖国际创新创业社区松山湖大学路16号教学楼508B，大湾区大学（松山湖校区） 
                <br>
			</p>

            <br>

		</div>
	</div>

	<div class="container">
		<h2>简介</h2>
        <p>王广聪加入了<a href="https://www.gbu.edu.cn/">大湾区大学</a>计算与信息科学技术学院担任助理教授。建立大湾区大学<a href="./team.html">视觉图形与交叉研究组</a>(Vision, Graphics, and X Group，VGX)。 </p>
        <p>从2021年4月到2024年4月，王广聪担任<a href="http://scse.ntu.edu.sg">南洋理工大学</a><a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>实验室博士后研究员，合作导师为<a href="https://liuziwei7.github.io/">Ziwei Liu</a>教授和<a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change (Cavan) Loy</a>教授。2020年12月，获得<a href="https://www.sysu.edu.cn/">中山大学</a>的<a href="https://cse.sysu.edu.cn/">计算机学院</a>博士学位，师从<a href="https://cse.sysu.edu.cn/content/2498">赖剑煌</a>教授，同时也受<a href="https://cse.sysu.edu.cn/content/2478">谢晓华</a>教授合作指导。</p>
        <p>研究兴趣是机器学习，深度学习，人工智能与科学技术。担任<a href="https://cvpr.thecvf.com/Conferences/2026/">CVPR 2026</a>，<a href="https://bmvc2024.org/">BMVC 2024</a>， <a href="https://bmvc2025.bmva.org/">2025</a>区域主席(Area Chair). 目前是TPAMI, IJCV, SIGGRAPH, ICML, NeurIPS, ICCV, CVPR, ECCV, ICLR, AAAI, ACM MM, TIP, TCSVT等期刊会议的审稿人。</p>
        <p>获得了<a href="http://www.cesexd.com/">中国电子教育学会</a>优秀博士学位论文。</p >
        <p>被评选为ICCV 2023杰出审稿人<a href="https://x.com/ICCVConference/status/1707400996228378992?s=20">Outstanding Reviewer</a>.
        <p>中国图象图形学会智能图形专业委员会委员 </p >
	    <p>他是<a href="https://thegreatailab.github.io/">The Great AI Lab</a>的成员。<font color="red"> 招聘博士后、博士生、硕士生、访问学生、实习生. 如果感兴趣可以发邮件联系. <a href="./recruit2024.html">详情见链接</a></font>.</p> 
	</div>


    <div class="container">    
        <h2>教学</h2>
        <h4> <li>生成式人工智能 (乔子越, 王广聪)</li></h4>
        <h4> <li>问题求解（包括编程、数据结构与算法）（王广聪，黄英豪）</li></h4>
    </div>      

	<div class="container">
	<h4>研究方向</h4>
	<li>3D/4D视觉: 3D/4D重建、生成、编辑、驱动  </li>
    <li>视频生成与理解：长视频理解，视频生成、编辑，包括Talking Face、数字人等 </li>
    <li>视觉感知与语言大模型</li>
    <li>具身智能</li>
    <li>AI for Science</li>

    <h4>研究目标</h4>
    <li>真实的虚拟3D世界系统，称为Open 3D Virtual Reality (<a href="https://open3dvr.github.io/index.html">Open3DVR</a>)</li>
    <li>智能生物技术系统, 称为Open Biotechnology and Machine Learning (<a href="https://openbiotechml.github.io/index.html">OpenBiotechML</a>)</li>
    
    </div> 
	<!-- <p>他有两个长期的目标。一个是利用机器学习建立一个真实的虚拟3D世界系统，称为Open 3D Virtual Reality (<a href="https://open3dvr.github.io/index.html">Open3DVR</a>)。另一个是利用机器学习建立一个生物技术系统来解决未来生物难题, 称为Open Biotechnology and Machine Learning (<a href="https://openbiotechml.github.io/index.html">OpenBiotechML</a>)。 </p>
	<p>Open3DVR和OpenBiotechML是非盈利的开放研究平台，关注领先的技术和造福于人类。</p>
	<p>生命短暂而璀璨。 他将为此目标花30年时间来发展相关技术。非常欢迎相关领域的研究员一起来研究这两个问题。</p>
	<p><em>"每一次成功的背后都有千百次失败"</em></p>
	<p><em>"我曾經跨過山和大海 也穿過人山人海</em></p>
        <p><em>我曾經擁有著一切 轉眼都飄散如煙</em></p>
        <p><em>我曾經失落失望失掉所有方向</em></p>
        <p><em>直到看見平凡 才是唯一的答案"</em></p>
	<p><a href="https://open3dvr.github.io/index.html"><img src="./homepage_files/open3dvr_logo.png" style="width:200px;height:50px;"></a> &nbsp&nbsp&nbsp <a href="https://openbiotechml.github.io/index.html"><img src="./homepage_files/openbiotechml_logo.png" style="width:300px;height:50px;"></a></p>
	</div> -->
	
	
    <div class="container">
    <h2>最新消息</h2>
    <p><font color='black'>[2025-09]</font> 担任计算机视觉与模式识别会议区域主席<a href="https://cvpr.thecvf.com/Conferences/2026/">Area Chair of CVPR 2026</a>.
    <p><font color='black'>[2025-08]</font> 担任<a href="https://mp.weixin.qq.com/s/7HPIiOzz34iXjQwFjzL8Dg">CSIG图像图形中国行-大湾区大学站执行主席</a>。
    <p><font color='black'>[2025-07]</font> <a href="https://free4d.github.io/">Free4D（ICCV 2025）</a>（项目页面、代码和演示）已发布。Free4D 是一种将单张图像转换为四维场景的新方法。欢迎使用我们的代码。
    <p><font color='black'>[2025-07]</font> <a href="https://zjhjojo.github.io/segmentdreamer/">SegmentDreamer（ICCV 2025）</a>（项目页面、代码和演示）已发布。SegmentDreamer 是一种新的方法，可根据文本生成三维物体。
    <p><font color='black'>[2025-05]</font> 担任<a href="https://www.csig.org.cn/16/201801/49334.html">中国图象图形学会智能图形专业委员会</a>委员. 委员会定位于推动计算机图形学向智能化、自动化方向发展。欢迎领域内的一流学者及学术新秀参加，进行学术交流和推动相关领域的研究进展及产学研合作。
    <p><font color='black'>[2025-05]</font> 担任英国机器学习会议区域主席<a href="https://bmvc2025.bmva.org/">(BMVC) 2025</a>.
    <!-- <p><font color='black'>[2025-03]</font> <a href="https://free4d.github.io/">Free4D (Arxiv'25)</a>  论文、项目主页、代码已公开。欢迎使用。 Free4D可以将一张图像转化成4D虚拟场景。欢迎使用我们的代码。 -->
    <p><font color='black'>[2025-03]</font> <a href="https://diffv2ir.github.io/">DiffV2IR (Arxiv'25)</a>  论文、项目主页、代码已公开。欢迎使用。DiffV2IR可以将RGB图像转换为红外图像。 欢迎使用我们的代码。
    <p><font color='black'>[2025-03]</font> <a href="https://arxiv.org/abs/2501.01645">HLV-1K (ICME'25)</a>  论文已公开。HLV-1K 包含了1千个视频和对应的标注，可用于视频理解分析.
    <p><font color='black'>[2025-02]</font> <a href="https://wildavatar.github.io/">WildAvatar (CVPR'25)</a>论文、项目主页、数据集、代码已公开。欢迎使用数据和代码工具。WildAvatar是YouTube上收集的大尺度数据集，包括10,000+人体，可以用来训练人体生成创建等, 这个数据集收集的目的是解决常见实验室中收集的数据集的局限性.
    <p><font color='black'>[2025-02]</font> <a href="https://narcissusex.github.io/GuardSplat/">GuardSplat (CVPR'25) </a> 论文、项目主页、代码已公开。欢迎使用。 GuardSplat是一种高效的3D水印方法，用来保护3D资产. 但某人使用你增加水印的3d资产时，你只需3d资产的任意视角的渲染图，即可解密出你独特的水印，不需要查看对方的3d资产。
    <p><font color='black'>[2024-11]</font>  <a href="https://mp.weixin.qq.com/s/dKwbs5TU8a4kAPjvofUlFw">西北工业大学邀请报告--3D场景创作与编辑</a>。
    <p><font color='black'>[2024-09]</font> <a href="https://arxiv.org/abs/2409.18938/">长视频理解综述 (Arxiv'24) </a> 公开.
    <p><font color='black'>[2024-09]</font> <a href="https://vcc.tech/index">深圳大学可视计算研究中心（VCC）</a> <a href="https://mp.weixin.qq.com/s/Thw0Soi6tcN2AlM5otfSZA">邀请报告</a>--3D场景创作与编辑.
    <p><font color='black'>[2024-08]</font> <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid++ （TPAMI'24）</a> 论文、项目主页、代码已公开。欢迎使用。Fast-Vid2Vid++是一种实时的视频转变的方法。例如将语义分割的视频转变为对应真实的视频。
    <p><font color='black'>[2024-05]</font> <a href="https://mvsgaussian.github.io/">MVSGaussian (ECCV'24)</a>论文、项目主页、代码已公开。欢迎使用。MVSGaussian是一种泛化性高斯，可以高效地重建3D场景。可以快速训练和实时渲染。
    <p><font color='black'>[2024-05]</font> 担任英国机器学习会议区域主席<a href="https://bmvc2024.org/"> (BMVC) 2024</a>。
    <p><font color='black'>[2024-04]</font> 我加入了<a href="https://www.gbu.edu.cn/">大湾区大学</a>，担任信息科学与技术学院tenure-track助理教授. <font color="black"> 即将招聘博士后、博士生、硕士生、研究助理、访问学生、实习生. 如果感兴趣可以发邮件联系. <a href="./recruit2024.html">详情见链接</a></font>.
    <p><font color='black'>[2024-04]</font> <a href="https://perf-project.github.io/">PERF (TPAMI'24)</a>已开源, PERF是一种可以将单张全景图变成一个3D场景的方法，应用场景有全景图转3D, 文本转3D, 命令提示的3D风格化，欢迎使用我们的代码.
	<p><font color='black'>[2023-12]</font> <a href="https://frozenburning.github.io/projects/primdiffusion/">PrimDiffusion (NeurIPS'23)</a> 已经发布. PrimDiffusion是一个3D人体生成模型。欢迎使用我们的代码.
	<p><font color='black'>[2023-09]</font> 非常高兴评选为ICCV 2023杰出审稿人<a href="https://x.com/ICCVConference/status/1707400996228378992?s=20">Outstanding Reviewer</a>.
	<p><font color='black'>[2023-07]</font> <a href="https://sparsenerf.github.io/">SparseNeRF (ICCV'23)</a>。SparseNeRF目标是从少量图片重建3D,合成新视角。代码已开源，请点击 <a href="https://github.com/Wanggcong/SparseNeRF">链接</a>查看。
	<p><font color='black'>[2023-04]</font> Program Chair of <a href="https://www.mmlab-ntu.com/cvpr_ntu/seminar_2023/index.html">Pre-CVPR@NTU</a>。</p>
	<p><font color='black'>[2023-04]</font> <a href="https://scene-dreamer.github.io/">SceneDreamer (TPAMI'23)</a>代码已公开。 快来试试创造你的世界吧! 点击<a href="https://huggingface.co/spaces/FrozenBurning/SceneDreamer">huggingface</a>。</p>
	<p><font color='black'>[2022-08]</font> <a href="https://frozenburning.github.io/projects/text2light/">Text2Light (TOG 2022, Proc. SIGGRAPH Asia, journal-track)</a>, 快来试试用文本生成你喜欢的HDR全景图吧! 点击 <a href="https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb">Colab</a>。</p>        <!-- <font color='red'>NEW!</font> --> 
	<p><font color='black'>[2022-07]</font> <a href="https://style-light.github.io/">StyleLight(ECCV'22)</a>和<a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid(ECCV'22)</a>。欢迎使用StyleLight生成HDR全景图给定LDR FOV图和使用Fast-Vid2Vid压缩video-to-video GAN模型 </p>  <!-- <font color='red'>NEW!</font> --> 
    <p><font color='black'>[2021-07]</font> <a href="https://github.com/wanggrun/triplet">Efficient Self-supervised Learning(ICCV'21,口头报告)</a>，高效的自监督模型，欢迎使用。</p>
    <p><font color='black'>[2021-06]</font> <a href="https://ieeexplore.ieee.org/document/9470916">Unsupervised Cross-modality Learning (TIP'21)</a>，首次尝试无监督跨模态行人识别。</p>
    <p><font color='black'>[2021-04]</font> 我加入<a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>实验室。</p>
    <p><font color='black'>[2021-03]</font> <a href="https://ieeexplore.ieee.org/document/9408408">Architecture Adaptation (TNNLS'21)</a>和<a href="https://arxiv.org/abs/2105.06714">Adaptive VSO (ICME'21, oral)</a>。</p>    
    <p><font color='black'>[2020-12]</font> 我正式被授予中山大学计算机学院博士学位 (<a href="https://mp.weixin.qq.com/s/D4F52P-ZXTloxoEXDkAV-g">中国电子教育学会优秀博士学位论文</a>和<a href="http://cse.sysu.edu.cn/content/6175">中山大学优秀博士论文</a>)。</p>  
    <p><font color='black'>[2020-11]</font> 媒体报道: 行人再识别大型数据集SYSU-30k已经发布, <a href="https://mp.weixin.qq.com/s/XsE0wBECzO6Mddtfdu8vVA">link</a>。数据集包含很多人物图像，适用于以人为主心的任务<br></p>
    </div>

       
    <div class="container">  
    <h2> 学术活动 </h2> 
    <h4> 计算机视觉与模式识别会议区域主席，Area Chair of Conference on Computer Vision and Pattern Recognition (CVPR), <a href="https://cvpr.thecvf.com/Conferences/2026">2026</a> </h4>
    <h4> 英国机器视觉区域主席，Area Chair of the British Machine Vision Conference (BMVC) <a href="https://bmvc2024.org/">2024</a> <a href="https://bmvc2025.bmva.org/">2025</a> </h4>
    <h4> Program Chair of <a href="https://www.mmlab-ntu.com/cvpr_ntu/seminar_2023/index.html">Pre-CVPR@NTU</a> </h4>
    <h4> <a href="https://mp.weixin.qq.com/s/7HPIiOzz34iXjQwFjzL8Dg">CSIG图像图形中国行-大湾区大学站执行主席</a></h4>
    <h4> <a href="https://www.csig.org.cn/16/201801/49334.html">中国图象图形学会智能图形专业委员会</a>委员 </h4>

    <h4>会议论文(审稿人)</h4>
    <li>SIGGRAPH </li>
    <li>SIGGRAPH Asia </li>
    <li>International Conference on Learning Representations (ICLR) </li>
    <li>International Conference on Machine Learning (ICML) </li>
    <li>Neural Information Processing Systems (NeurIPS)</li>    
    <li>International Conference on Computer Vision (ICCV) (<a href="https://x.com/ICCVConference/status/1707400996228378992?s=20">杰出审稿人</a>) </li>
    <li>Computer Vision and Pattern Recognition (CVPR) </li>
    <li>European Conference on Computer Vision (ECCV) </li>
    <li>Association for the Advancement of Artificial Intelligence (AAAI) </li>
    <li>Others: ACM MM, ICPR, ACCV </li>

    <h4>期刊论文(审稿人)</h4>
    <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </li>
    <li>International Journal of Computer Vision (IJCV) </li>
    <li>IEEE Transactions on Visualization and Computer Graphics (TVCG)</li>
    <li>IEEE Transactions on Image Processing (TIP) </li>
    <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) </li>
    <li>Others: TOMM, Neurocomputing, SPL, etc.
    <!-- <li>Others: TOMM, Neurocomputing, SPL, The Visual Computer, Image and Vison Computing -->
     <!-- <li>Others: TOMM, Neurocomputing,Signal Processing Letters, The Visual Computer, Image and Vison Computing -->
    </div>

 

    <div class="container">
	<h2>代表工作  (<a href="./publications.html">完整列表</a>)</h2>
	<p>*共同一作，+同等通讯作者</p>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/Style4D_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://becky-catherine.github.io/Style4D/">Style4D-Bench: A Benchmark Suite for 4D Stylization</a>
            </strong>
            <br>
            Beiqi Chen*, Shuai Shao*, Haitang Feng, Jianhuang Lai, Jianlou Si+, <b>Guangcong Wang</b>+
            <br>
            <em>Arxiv, 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2508.19243">PDF</a>
                <a href="https://becky-catherine.github.io/Style4D/">Project Page</a>
                <a href="https://github.com/Becky-catherine/Style4D-Bench">Code</a> 
                <a href="https://www.youtube.com/watch?v=Tf6QnksXFxQ">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>     
    
    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/objfiller3D_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://objfiller3d.github.io/">ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models</a>
            </strong>
            <br>
            Haitang Feng, Jie Liu+, Jie Tang, Gangshan Wu, Beiqi Chen, Jianhuang Lai, <b>Guangcong Wang</b>+
            <br>
            <em>Arxiv, 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2508.18271">PDF</a>
                <a href="https://objfiller3d.github.io/">Project Page</a>
                <a href="https://github.com/objfiller3d/ObjFiller-3D">Code</a> 
                <a href="https://www.youtube.com/watch?v=0YZoKtpvqHY">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>     


    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/Distilled_3DGS_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://distilled3dgs.github.io/">Distilled-3DGS: Distilled 3D Gaussian Splatting</a>
            </strong>
            <br>
            Lintao Xiang<sup>*</sup>, Xinkai Chen<sup>*</sup>, Jianhuang Lai, <b>Guangcong Wang</b>.
            <br>
            <em>Arxiv, 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2508.14037">PDF</a>
                <a href="https://distilled3dgs.github.io/">Project Page</a>
                <a href="https://github.com/lt-xiang/Distilled-3DGS">Code</a> 
                <a href="https://distilled3dgs.github.io/">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>  


    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/free4D_logo.gif" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://free4d.github.io/">Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency</a>
            </strong>
            <br>
            Tianqi Liu<sup>*</sup>, Zihao Huang<sup>*</sup>, Zhaoxi Chen, <b>Guangcong Wang</b>, Shoukang Hu, Liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li<sup>+</sup>, Ziwei Liu<sup>+</sup>.
            <br>
            <em>International Conference on Computer Vision (ICCV), 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2503.20785">PDF</a>
                <a href="https://free4d.github.io/">Project Page</a>
                <a href="https://github.com/TQTQliu/Free4D">Code</a> 
                <a href="https://www.youtube.com/watch?v=GpHnoSczlhA&feature=youtu.be">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/SegmentDreamer_logo.gif" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://zjhjojo.github.io/segmentdreamer/">SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillationy</a>
            </strong>
            <br>
            Jiahao Zhu, Zixuan Chen, <b>Guangcong Wang</b>, Xiaohua Xie, Yi Zhou.
            <br>
            <em>International Conference on Computer Vision (ICCV), 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/pdf/2507.05256v1">PDF</a>
                <a href="https://zjhjojo.github.io/segmentdreamer/">Project Page</a>
                <a href="https://github.com/zjhJOJO/SegmentDreamer">Code</a> 
                <a href="https://www.youtube.com/watch?v=haUjuRgiis0">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>


    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/DiffV2IR_logo.gif" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://diffv2ir.github.io/">DiffV2IR: Visible-to-Infrared Diffusion Model via Vision-Language Understanding</a>
            </strong>
            <br>
            Linyan Ran, Lidong Wang, <b>Guangcong Wang</b>, Peng Wang, Yangning Zhang.
            <br>
            <em>Arxiv, 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2503.19012">PDF</a>
                <a href="https://diffv2ir.github.io/">Project Page</a>
                <a href="https://github.com/LidongWang-26/DiffV2IR">Code</a> 
                <a href="https://www.youtube.com/watch?v=YbUuvjnfejE&feature=youtu.be">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/HLV-1K_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://arxiv.org/abs/2501.01645">HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding</a>
            </strong>
            <br>
            Heqing Zou, Tianze Luo, Guiyang Xie, Victor (Xiao Jie)Zhang, Fengmao Lv, <b>Guangcong Wang</b>, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang
            <br>
            <em>IEEE International Conference on Multimedia and Expo (ICME), 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2501.01645">PDF</a>
                <!-- <a href="https://diffv2ir.github.io/">Project Page</a> -->
                <!-- <a href="https://github.com/LidongWang-26/DiffV2IR">Code</a>  -->
                <!-- <a href="https://www.youtube.com/watch?v=YbUuvjnfejE&feature=youtu.be">Demo</a> -->
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/wildavatar_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://wildavatar.github.io/">WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation</a>
            </strong>
            <br>
            Zihao Huang, Shoukang Hu, <b>Guangcong Wang</b>, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.
            <br>
            <em>Computer Vision and Pattern Recognition Conference (CVPR), 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/pdf/2407.02165v2">PDF</a>
                <a href="https://wildavatar.github.io/">Project Page</a>
                <a href="https://github.com/wildavatar/WildAvatar_Toolbox">Code</a> 
                <a href="https://www.youtube.com/watch?v=T-XafMVKY7E">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>
    
    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/guard-splat_logo.gif" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://narcissusex.github.io/GuardSplat/">GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting</a>
            </strong>
            <br>
            Zixuan Chen, <b>Guangcong Wang</b>, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie.
            <br>
            <em>Computer Vision and Pattern Recognition Conference (CVPR), 2025</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2411.19895">PDF</a>
                <a href="https://narcissusex.github.io/GuardSplat/">Project Page</a>
                <a href="https://github.com/NarcissusEx/GuardSplat">Code</a> 
                <a href="https://youtu.be/QgejiJE2-5g">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>       

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/vividreamer_logo.png" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://narcissusex.github.io/VividDreamer/">VividDreamer: Towards High-Fidelity and Efficient Text-to-3D Generation</a>
            </strong>
            <br>
            Zixuan Chen, Ruijie Su, Jiahao Zhu, <b>Guangcong Wang</b>, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie
            <br>
            <em>Arxiv, 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2406.14964">PDF</a>
                <a href="https://narcissusex.github.io/VividDreamer/">Project Page</a>
                <a href="https://github.com/NarcissusEx/VividDreamer">Code</a> 
                <a href="https://narcissusex.github.io/VividDreamer/">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/long_video_understanding_logo.png" class="publogo4" width="200 px">
        <p> 
            <strong>
                <a href="https://arxiv.org/abs/2409.18938">From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding</a>
            </strong>
            <br>
            Heqing Zou, Tianze Luo, Guiyang Xie, Victor Zhang, Fengmao Lv, <b>Guangcong Wang</b>, Juanyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang
            <br>
            <em>Arxiv, 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2409.18938">PDF [Survey]</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/fast-vid2vid++_logo.png" class="publogo" width="200 px">
        <p> 
            <strong>
                <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid++: Spatial-Temporal Distillation for Real-Time Video-to-Video Synthesis</a>
            </strong>
            <br>
            Long Zhuo, <b>Guangcong Wang</b>, Shikai Li, Wayne Wu, Ziwei Liu.
            <br>
            <em> IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2207.05049">PDF</a>
                <a href="https://fast-vid2vid.github.io/">Project Page</a>
                <a href="https://github.com/fast-vid2vid/fast-vid2vid">Code</a>
                <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

    <div class="publication">
        <img style=”float: left; padding: 0px 0px 30px 0px;” src="./homepage_files/mvsgaussian_logo.gif" class="publogo2" width="200 px">
        <p> 
            <strong>
                <a href="https://mvsgaussian.github.io/">MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo</a>
            </strong>
            <br>
            Tianqi Liu, <b>Guangcong Wang</b>, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.
            <br>
            <em>European Conference on Computer Vision (ECCV), 2024</em>
            <br>
            <span class="links">
                <a href="https://arxiv.org/abs/2405.12218">PDF</a>
                <a href="https://mvsgaussian.github.io/">Project Page</a>
                <a href="https://github.com/TQTQliu/MVSGaussian">Code</a>
                <a href="https://m.youtube.com/watch?v=yzYVY7apyJE&feature=youtu.be">Demo</a>
            </span>
        </p>
    </div>
    <br>
    <br>
    <br>

        <div class="publication">
            <img src="./homepage_files/perf_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://perf-project.github.io/">PERF: Panoramic Neural Radiance Field from a Single Panorama</a>
                </strong>
		<br>
		<b>Guangcong Wang</b><sup>*</sup>, Peng Wang<sup>*</sup>, Zhaoxi Chen, Wenping Wang, Chen Change Loy, Ziwei Liu
		<br>
                <em> IEEE Transactions on Pattern Analysis and Machine Intelligence(TPAMI), 2024</em>
		<br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2310.16831">PDF</a>
                    <a href="https://perf-project.github.io/">Project Page</a>
                    <a href="https://github.com/perf-project/PeRF">Code</a>
                    <a href="https://www.youtube.com/watch?v=4wa2h1fjh2U&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
        <div class="publication">
            <img src="./homepage_files/primdiffusion_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://frozenburning.github.io/projects/primdiffusion/">PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation</a>
                </strong>
		<br>
		Zhaoxi Chen, Fangzhou Hong, Haiyi Mei, <b>Guangcong Wang</b>, Lei Yang, Ziwei Liu
		<br>
                <em> Neural Information Processing Systems (NeurIPS), 2023</em>
		<br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2312.04559">PDF</a>
                    <a href="https://frozenburning.github.io/projects/primdiffusion/">Project Page</a>
                    <a href="https://github.com/FrozenBurning/PrimDiffusion">Code</a>
                    <a href="https://www.youtube.com/watch?v=zprHGZ7Gm7A&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>	    
	    
        <div class="publication">
            <img src="./homepage_files/sparsenerf_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://https://sparsenerf.github.io/">SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis</a>
                </strong>
                <br>
                <b>Guangcong Wang</b>, Zhaoxi Chen, Chen Change Loy, Ziwei Liu.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2023</em>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2303.16196">PDF</a>
                    <a href="https://sparsenerf.github.io/">Project Page</a> 
                    <a href="https://github.com/Wanggcong/SparseNeRF">Code</a>
                    <a href="https://www.youtube.com/watch?v=V0yCTakA964&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
        <div class="publication">
            <img src="./homepage_files/scenedreamer_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://scene-dreamer.github.io/">SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections</a>
                </strong>
                <br>
                Zhaoxi Chen, <b>Guangcong Wang</b>, Ziwei Liu.
                <br>
                <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023</em>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2302.01330">PDF</a>
                    <a href="https://scene-dreamer.github.io/">Project Page</a>
                    <a href="https://github.com/Scene-Dreamer/SceneDreamer">Code</a>
                    <a href="https://www.youtube.com/watch?v=AjtOlDHsiyU">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
        <div class="publication">
            <img src="./homepage_files/text2light_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://frozenburning.github.io/projects/text2light/">Text2Light: Zero-Shot Text-Driven HDR Panorama Generation</a>
                </strong>
                <br>
                <br>
                Zhaoxi Chen,  <b>Guangcong Wang</b>,Ziwei Liu.
                <br>
                <em>ACM Transactions on Graphics (SIGGRAPH Asia), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2209.09898">PDF</a>
                    <a href="https://frozenburning.github.io/projects/text2light/">Project Page</a>
                    <a href="https://github.com/FrozenBurning/Text2Light">Code</a>
                    <a href="https://www.youtube.com/watch?v=XDx6tOHigPE">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
	<br>
         <div class="publication">
            <img src="./homepage_files/stylelight_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://style-light.github.io/">StyleLight: HDR Panorama Generation for Lighting Estimation and Editing</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Yinuo Yang, Chen Change Loy, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2207.14811">PDF</a>
                    <a href="https://style-light.github.io/">Project Page</a>
                    <a href="https://github.com/Wanggcong/StyleLight">Code</a>
                    <a href="https://www.youtube.com/watch?v=sHeWK1MSPg4">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	         

        <div class="publication">
            <img src="./homepage_files/fastvid2vid_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis</a>
                </strong>
                <br>
                <br>
                Long Zhuo, <b>Guangcong Wang</b>, Shikai Li, Wayne Wu, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2207.05049">PDF</a>
                    <a href="https://fast-vid2vid.github.io/">Project Page</a>
                    <a href="https://github.com/fast-vid2vid/fast-vid2vid">Code</a>
                    <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 
        
        <div class="publication">
            <img src="./homepage_files/triplet_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/wanggrun/triplet">Solving Inefficiency of Self-supervised Representation Learning</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, Keze Wang, <b>Guangcong Wang</b>, Philip Torr, Liang Lin.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Solving_Inefficiency_of_Self-Supervised_Representation_Learning_ICCV_2021_paper.pdf">PDF</a>
                    <a href="https://github.com/wanggrun/triplet">Code</a>
                    <a href="https://arxiv.org/abs/2104.08760">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/h2h_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9470916">Homogeneous-to-Heterogeneous:  Unsupervised Learning for  RGB-Infrared Person Re-Identification</a>
                </strong>
                <br>
                <br>
                Wenqi Liang, <b>Guangcong Wang</b>, Jianhuang Lai, Xiaohua Xie.
                <br>
                <em>IEEE Transactions on Image Processing (TIP), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9470916">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/Joint Learning_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9408408">Joint Learning of Neural Transfer and Architecture Adaptation for Image Recognition</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, Liang Lin, Rongcong Chen, <b>Guangcong Wang</b>, Jiqi Zhang.
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9408408">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         


        <div class="publication">
            <img src="./homepage_files/Smoothing_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">Smoothing Adversarial Domain Attack and p-Memory Reconsolidation for Cross-Domain Person Re-Identification</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Wenqi Liang, Guangrun Wang.
                <br>
                <em>Computer Vision and Pattern Recognition Conference (CVPR) , 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         
    
        <div class="publication">
            <img src="./homepage_files/weaklyreid_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/wanggrun/SYSU-30k">Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, <b>Guangcong Wang</b>, Xujie Zhang, Jianhuang Lai, Liang Lin.
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/1904.03845">PDF</a>
                    <a href="https://github.com/wanggrun/SYSU-30k">Dataset, Code, Pretrained model</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 


        <div class="publication">
            <img src="./homepage_files/treeconv_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://www.kdd.org/kdd2020/accepted-papers/view/grammatically-recognizing-images-with-tree-convolution">Grammatically Recognizing Images with Tree Convolution</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, <b>Guangcong Wang</b>, Keze Wang, Xiaodan Liang, Liang Lin.
                <br>
                <em>ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://www.kdd.org/kdd2020/accepted-papers/view/grammatically-recognizing-images-with-tree-convolution">PDF</a>
                    <a href="https://github.com/wanggrun/TreeConv/stargazers">Code</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>   

        <div class="publication">
            <img src="./homepage_files/streid_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Spatial-Temporal Person Re-identification</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Peigen Huang and Xiaohua Xie.
                <br>
                <em>The Association for the Advancement of Artificial Intelligence (AAAI), 2019</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4921">PDF</a>
                    <a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Code</a>
                    <a href="https://arxiv.org/abs/1812.03282">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

      
        <div class="publication">
            <img src="./homepage_files/P2SNet_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8025424">P2SNet: Can an Image Match a Video for Person Re-identification in an End-to-end Way</a>
                </strong>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Xiaohua Xie.
                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</em>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8025424">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         

        <div class="publication">
            <img src="./homepage_files/DGL_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf">Deep Growing Learning</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Xiaohua Xie, Jianhuang Lai, Jiaxuan Zhuo.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2017</em>
                <br>
                <br>
                <span class="links">
                    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf">PDF</a>
                    <a href="https://github.com/Wanggcong/Deep-growing-learning">Code</a>
                    
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         

	<div style="text-align:center">
            <a href="https://clustrmaps.com/site/1bw2p"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=DEKzN_4t5lNck7an-VfV9GZNqykDXeskr7_d56i0Qhw" /></a>
        </div>
	    
        <div style="text-align:right">
            Website template credits to <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        </div>
	

	
	<!-- <div style="text-align:right">
            <img src="https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2FWanggcong%2FWanggcong.github.io%2Fblob%2Fmaster%2Findex.html&label=stats&countColor=%23263759" /></a>
        </div>		
	</div> -->
	
</body></html>
