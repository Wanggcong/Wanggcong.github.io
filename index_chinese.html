
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Guangcong Wang; 王广聪; 计算机视觉; 深度学习; 中山大学; 南洋理工大学; SYSU; NTU">
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.min.js"></script>
<link rel="author" href="https://wanggcong.github.io/">

    <title>Guangcong Wang - Homepage</title>
    <style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : rgb(224, 224, 224); }
.title { width : 650px; margin : 20px auto; }
.container { width : 750px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
.container_title { width : 750px; margin : 20px auto; border-radius: 10px;  padding : 20px;  clear:both;}
.iframe_video {float: left; margin-right: 30px}
#bio {
    padding-top : 20px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; margin-right : 100px; border : 0 solid black; float : left; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
	<script async="" src="./homepage.js"></script>
</head>

<body>

    <div class="container">
        <h2>
            <a href="./index_chinese.html">主页</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./competitions.html">比赛</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./publications.html">出版</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="./datasets.html">数据集</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://www.mmlab-ntu.com/">团队</a>&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://github.com/wanggcong/">Github代码</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://scholar.google.com/citations?user=dk8EnkoAAAAJ&hl=en">谷歌学术</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://wanggrun.github.io/">相关链接</a>&nbsp;&nbsp;&nbsp;&middot;&nbsp;&nbsp;
            <a href="https://Wanggcong.github.io/index">English Version</a><br>
        </h2>
    </div>

	<div class="container">
		<div id="sidebar">
            <img src="./wanggcong.jpeg" vspace="10 px" width="180 px" id="me" itemprop="photo">
        </div>
		<div id="bio">

            <br>

			<h2>
				<span itemprop="name">王广聪 <font size="4">  Guangcong Wang</font> </span>
			</h2>

            <br>

			<p style="line-height:23px;">
				博士后研究员
                <br>
                计算机科学与工程学院
                <br>
                南洋理工大学
                <br>
                邮箱: wanggc3 at gmail.com or guangcong.wang at ntu.edu.sg
                <br>
                地址: ABN-02b-14, S-LAB, Nanyang Avenue  
                <br>
			</p>

            <br>

		</div>
	</div>

	<div class="container">
		<h2>简介</h2>

        <p>王广聪目前是<a href="http://scse.ntu.edu.sg">南洋理工大学</a>的<a href="http://scse.ntu.edu.sg">计算机科学与工程学院</a>的一名博士后研究员，合作导师为<a href="https://liuziwei7.github.io/">Ziwei Liu</a>教授和<a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change (Cavan) Loy</a>教授。在此之前，他是<a href="http://www.sysu.edu.cn/cn/index.htm">中山大学</a>的<a href="http://sdcs.sysu.edu.cn/">计算机学院</a>的一名博士生，师从<a href="http://sdcs.sysu.edu.cn/content/2498">赖剑煌</a>教授，同时也受<a href="http://sdcs.sysu.edu.cn/content/2478">谢晓华</a>教授合作指导。</p>
        <p>他的研究兴趣是生成模型，3D模型，行人再识别，半监督学习，神经网络可解释性和元学习。他已经发表若干篇论文，包括TOG, IEEE T-NNLS, IEEE TIP, IEEE T-CSVT, ICCV, CVPR, ECCV, KDD, AAAI。</p>
        <p>他目前是IJCV, ICML, NeurIPS, ICCV, CVPR, ECCV, ICLR, AAAI, ICPR, IEEE TIP, IEEE TOMM, IEEE T-CSVT, IEEE Signal Processing Letters, Neurocomputing等期刊会议的审稿人。</p>
        <p>他获得了<a href="http://www.cesexd.com/">中国电子教育学会</a>优秀博士学位论文。</p >
	<p>他加入了<a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>实验室。 他是<a href="https://wanggrun.github.io/projects/fast">FAST Lab</a>的成员。</p>

	</div>

	<div class="container">
	<h2>目标</h2>
		
	<p>他有两个长期的目标。一个是利用机器学习建立一个真实的虚拟3D世界系统，称为Open 3D Virtual Reality (<a href="https://open3dvr.github.io/index.html">Open3DVR</a>)。另一个是利用机器学习建立一个生物技术系统来解决未来生物难题, 称为Open Biotechnology and Machine Learning (<a href="https://openbiotechml.github.io/index.html">OpenBiotechML</a>)。 </p>
	<p>Open3DVR和OpenBiotechML是非盈利的开放研究平台，关注领先的技术和造福于人类。</p>
	<p>生命短暂而璀璨。 他将为此目标花30年时间来发展相关技术。非常欢迎相关领域的研究员一起来研究这两个问题。</p>
	<p><em>每一次成功的背后都有千百次失败.</em></p>
	<p><a href="https://open3dvr.github.io/index.html"><img src="./logo.jpeg" style="width:50px;height:50px;"></a> &nbsp&nbsp&nbsp <a href="https://open3dvr.github.io/index.html"><img src="./homepage_files/open3dvr_logo.png" style="width:200px;height:50px;"></a> &nbsp&nbsp&nbsp <a href="https://openbiotechml.github.io/index.html"><img src="./homepage_files/openbiotechml_logo.png" style="width:300px;height:50px;"></a></p>
	</div>
	
	
    <div class="container">
        <h2>最新消息</h2>
	<p><font color='black'>[2023-04]</font> Program Chair of <a href="https://www.mmlab-ntu.com/cvpr_ntu/seminar_2023/index.html">Pre-CVPR@NTU</a>。</p>
	<p><font color='black'>[2023-04]</font> <a href="https://scene-dreamer.github.io/">SceneDreamer</a>代码已公开。 快来试试创造你的世界吧! 点击<a href="https://huggingface.co/spaces/FrozenBurning/SceneDreamer">huggingface</a>。</p>
	<p><font color='black'>[2022-08]</font> 一篇TOG2022（Proc. SIGGRAPH Asia, journal-track）已经被接收，<a href="https://frozenburning.github.io/projects/text2light/">Text2Light</a>, 快来试试用文本生成你喜欢的HDR全景图吧! 点击 <a href="https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb">Colab</a>。</p>        <!-- <font color='red'>NEW!</font> --> 
	<p><font color='black'>[2022-07]</font> 两篇ECCV2022已经被接收, <a href="https://style-light.github.io/">StyleLight</a>和<a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid</a>。欢迎使用StyleLight生成HDR全景图给定LDR FOV图和使用Fast-Vid2Vid压缩video-to-video GAN模型 </p>  <!-- <font color='red'>NEW!</font> --> 
        <p><font color='black'>[2021-07]</font> 一篇ICCV2021(口头报告)已经被接收，<a href="https://github.com/wanggrun/triplet">Efficient Self-supervised Learning</a>，高效的自监督模型，欢迎使用。</p>
        <p><font color='black'>[2021-06]</font> 一篇TIP期刊论文已经被接收，<a href="https://ieeexplore.ieee.org/document/9470916">Unsupervised Cross-modality Learning</a>，首次尝试无监督跨模态行人识别。</p>
        <p><font color='black'>[2021-04]</font> 我加入<a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>实验室。</p>
        <p><font color='black'>[2021-03]</font> 一篇T-NNLS期刊论文和一篇ICME-2021（口头报告） 已经被接收，<a href="https://ieeexplore.ieee.org/document/9408408">Architecture Adaptation</a>和<a href="https://arxiv.org/abs/2105.06714">Adaptive VSO</a>。</p>    
        <p><font color='black'>[2020-12]</font> 我正式被授予中山大学计算机学院博士学位 (<a href="https://mp.weixin.qq.com/s/D4F52P-ZXTloxoEXDkAV-g">中国电子教育学会优秀博士学位论文</a>和<a href="http://cse.sysu.edu.cn/content/6175">中山大学优秀博士论文</a>)。</p>  
        <p><font color='black'>[2020-11]</font> 媒体报道: 行人再识别大型数据集SYSU-30k已经发布, <a href="https://mp.weixin.qq.com/s/XsE0wBECzO6Mddtfdu8vVA">link</a>。数据集包含很多人物图像，适用于以人为主心的任务<br></p>
    </div>

       
    <div class="container">  
    <h2> 学术活动 </h2> 
    <h4> Program chair of <a href="https://www.mmlab-ntu.com/cvpr_ntu/seminar_2023/index.html">Pre-CVPR@NTU</a> </h4>
    <h4>会议论文(审稿人)</h4>
    <li>ICLR: 2022, 2023 </li>
    <li>ICML: 2021, 2022, 2023 </li>
    <li>NeurIPS: 2020, 2021, 2022, 2023</li>    
    <li>ICCV: 2021, 2023 </li>
    <li>CVPR: 2021，2022, 2023 </li>
    <li>ECCV: 2022 </li>
    <li>AAAI: 2020, 2021, 2022, 2023 </li>
    <li>ICPR: 2020，2022 </li>
    <h4>期刊论文(审稿人)</h4>
    <li>IJCV </li>
    <li>IEEE TIP </li>
    <li>IEEE TOMM </li>
    <li>IEEE TCSVT </li>
    <li>Neurocomputing </li>
    <li>IEEE Signal Processing Letters </li>
    <li>The Visual Computer</li>
    <li>Image and Vison Computing</li>
    </div>

	
    <div class="container">
    <h2>教学与合作指导</h2>   
    <li>Yinuo Yang, 人工智能科学硕士, 南洋理工大学， Neural Perception and Reconstruction for Virtual Humans, 2021</li>
    <li>Liang Tong, 本科毕设, 南洋理工大学，Editing Anime Images using Generative Adversarial Network, 2021</li>
    <li>Long Zhuo, Intern, 商汤，GAN compression, 2021</li>
    <li>梁文琦, 硕士, 中山大学, Unsupervised Single-modality and Cross-modality Person Re-identification, <a href="http://cse.sysu.edu.cn/content/6175">优秀硕士论文</a>, 2021</li>	    
    <li>张恩齐, 梁文琦（优秀学士论文）, 周阳, 徐立, 刘秉运等, 本科毕设, 中山大学，2016-2020</li>   
    <li>统计分析方法，助教，2018</li>
    <li>信号与系统, 助教, 2018</li>
    <li>数字图像处理, 助教, 2017</li>
    </div>

    <div class="container">
	<h2>代表工作</h2>
	    
        <div class="publication">
            <img src="./homepage_files/sparsenerf_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://https://sparsenerf.github.io/">SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis</a>
                </strong>
                <br>
                <b>Guangcong Wang</b>, Zhaoxi Chen, Chen Change Loy, Ziwei Liu.
                <br>
                <em>Technical Report, 2023</em>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2303.16196">PDF</a>
                    <a href="https://sparsenerf.github.io/">Project Page</a> 
                    <a href="https://github.com/Wanggcong/SparseNeRF">Code</a>
                    <a href="https://www.youtube.com/watch?v=V0yCTakA964&feature=youtu.be">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
        <div class="publication">
            <img src="./homepage_files/scenedreamer_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://scene-dreamer.github.io/">SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections</a>
                </strong>
                <br>
                <br>
                Zhaoxi Chen, <b>Guangcong Wang</b>, Ziwei Liu.
                <br>
                <em>ArXiv, 2023</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2302.01330">PDF</a>
                    <a href="https://scene-dreamer.github.io/">Project Page</a>
                    <a href="https://github.com/Scene-Dreamer/SceneDreamer">Code</a>
                    <a href="https://www.youtube.com/watch?v=AjtOlDHsiyU">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	    
        <div class="publication">
            <img src="./homepage_files/text2light_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://frozenburning.github.io/projects/text2light/">Text2Light: Zero-Shot Text-Driven HDR Panorama Generation</a>
                </strong>
                <br>
                <br>
                Zhaoxi Chen,  <b>Guangcong Wang</b>,Ziwei Liu.
                <br>
                <em>ACM Transactions on Graphics (SIGGRAPH Asia), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2209.09898">PDF</a>
                    <a href="https://frozenburning.github.io/projects/text2light/">Project Page</a>
                    <a href="https://github.com/FrozenBurning/Text2Light">Code</a>
                    <a href="https://www.youtube.com/watch?v=XDx6tOHigPE">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
	<br>
         <div class="publication">
            <img src="./homepage_files/stylelight_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://style-light.github.io/">StyleLight: HDR Panorama Generation for Lighting Estimation and Editing</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Yinuo Yang, Chen Change Loy, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2207.14811">PDF</a>
                    <a href="https://style-light.github.io/">Project Page</a>
                    <a href="https://github.com/Wanggcong/StyleLight">Code</a>
                    <a href="https://www.youtube.com/watch?v=sHeWK1MSPg4">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>
	         

        <div class="publication">
            <img src="./homepage_files/fastvid2vid_logo.gif" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis</a>
                </strong>
                <br>
                <br>
                Long Zhuo, <b>Guangcong Wang</b>, Shikai Li, Wayne Wu, Ziwei Liu.
                <br>
                <em>European Conference on Computer Vision (ECCV), 2022</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2207.05049">PDF</a>
                    <a href="https://fast-vid2vid.github.io/">Project Page</a>
                    <a href="https://github.com/fast-vid2vid/fast-vid2vid">Code</a>
                    <a href="https://www.youtube.com/watch?v=AhEqjGVuk4A">Demo</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 
        
        <div class="publication">
            <img src="./homepage_files/triplet_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/wanggrun/triplet">Solving Inefficiency of Self-supervised Representation Learning</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, Keze Wang, <b>Guangcong Wang</b>, Philip Torr, Liang Lin.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Solving_Inefficiency_of_Self-Supervised_Representation_Learning_ICCV_2021_paper.pdf">PDF</a>
                    <a href="https://github.com/wanggrun/triplet">Code</a>
                    <a href="https://arxiv.org/abs/2104.08760">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/h2h_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9470916">Homogeneous-to-Heterogeneous:  Unsupervised Learning for  RGB-Infrared Person Re-Identification</a>
                </strong>
                <br>
                <br>
                Wenqi Liang, <b>Guangcong Wang</b>, Jianhuang Lai, Xiaohua Xie.
                <br>
                <em>IEEE Transactions on Image Processing (TIP), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9470916">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/Joint Learning_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/9408408">Joint Learning of Neural Transfer and Architecture Adaptation for Image Recognition</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, Liang Lin, Rongcong Chen, <b>Guangcong Wang</b>, Jiqi Zhang.
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/9408408">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         

        <div class="publication">
            <img src="./homepage_files/Confidence_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://arxiv.org/abs/2105.06714">Confidence-Guided Adaptive Gate and Dual Differential Enhancement for Video Salient Object Detection</a>
                </strong>
                <br>
                <br>
                Peijia Chen, Jianhuang Lai, <b>Guangcong Wang</b>, Xiaohua Xie.
                <br>
                <em>IEEE International Conference on Multimedia and Expo (ICME), 2021 (oral)</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2105.06714">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

        <div class="publication">
            <img src="./homepage_files/Smoothing_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">Smoothing Adversarial Domain Attack and p-Memory Reconsolidation for Cross-Domain Person Re-Identification</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Wenqi Liang, Guangrun Wang.
                <br>
                <em>Computer Vision and Pattern Recognition Conference (CVPR) , 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         
    
        <div class="publication">
            <img src="./homepage_files/weaklyreid_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/wanggrun/SYSU-30k">Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, <b>Guangcong Wang</b>, Xujie Zhang, Jianhuang Lai, Liang Lin.
                <br>
                <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/1904.03845">PDF</a>
                    <a href="https://github.com/wanggrun/SYSU-30k">Dataset, Code, Pretrained model</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 


        <div class="publication">
            <img src="./homepage_files/treeconv_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://www.kdd.org/kdd2020/accepted-papers/view/grammatically-recognizing-images-with-tree-convolution">Grammatically Recognizing Images with Tree Convolution</a>
                </strong>
                <br>
                <br>
                Guangrun Wang, <b>Guangcong Wang</b>, Keze Wang, Xiaodan Liang, Liang Lin.
                <br>
                <em>ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD), 2020</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://www.kdd.org/kdd2020/accepted-papers/view/grammatically-recognizing-images-with-tree-convolution">PDF</a>
                    <a href="https://github.com/wanggrun/TreeConv/stargazers">Code</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>   

        <div class="publication">
            <img src="./homepage_files/streid_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Spatial-Temporal Person Re-identification</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Peigen Huang and Xiaohua Xie.
                <br>
                <em>The Association for the Advancement of Artificial Intelligence (AAAI), 2019</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4921">PDF</a>
                    <a href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">Code</a>
                    <a href="https://arxiv.org/abs/1812.03282">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 

      

        <div class="publication">
            <img src="./homepage_files/m2m_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://arxiv.org/pdf/1811.03768.pdf">M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification</a>
                </strong>
                <br>
                <br>
                Wenqi Liang, <b>Guangcong Wang</b>, Jianhuang Lai.
                <br>
                <em>ACTA AUTOMATICA SINICA, 2019</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/1811.03768.pdf">arXiv</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br> 
        <div class="publication">
            <img src="./homepage_files/P2SNet_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8025424">P2SNet: Can an Image Match a Video for Person Re-identification in an End-to-end Way</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Jianhuang Lai, Xiaohua Xie.
                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018</em>
                <br>
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8025424">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         

        <div class="publication">
            <img src="./homepage_files/DGL_logo.png" class="publogo" width="200 px">
            <p> 
                <strong>
                    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf">Deep Growing Learning</a>
                </strong>
                <br>
                <br>
                <b>Guangcong Wang</b>, Xiaohua Xie, Jianhuang Lai, Jiaxuan Zhuo.
                <br>
                <em>International Conference on Computer Vision (ICCV), 2017</em>
                <br>
                <br>
                <span class="links">
                    <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf">PDF</a>
                    <a href="https://github.com/Wanggcong/Deep-growing-learning">Code</a>
                    
                </span>
            </p>
        </div>
        <br>
        <br>
        <br>         
         
        <div style="text-align:right">
            Website template credits to <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        </div>
	

	
	<div style="text-align:right">
            <img src="https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2FWanggcong%2FWanggcong.github.io%2Fblob%2Fmaster%2Findex.html&label=stats&countColor=%23263759" /></a>
        </div>		
	</div>
	
</body></html>
