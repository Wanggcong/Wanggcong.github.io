| **Paper 31**          |                                                              |
| --------------------- | ------------------------------------------------------------ |
| Title:                | Meta Transfer Learning by Neural Architecture Adaptation     |
| Paper:                | [![KDD2019_paper_31.pdf](https://easychair.org/images/uploads/open.gif)](https://easychair.org/conferences/submission_download?track=230451;upload=85726;submission=4189628;a=20899107)(Feb 28, 00:05 GMT) |
| Track:                | Research                                                     |
| Author keywords:      | Deep learningNeural networksNeural architecture adaptationTransfer learning |
| EasyChair keyphrases: | transfer learning (1286), meta transfer learning (1172), standard transfer learning (660), meta transfer (565), network architecture (340), standard transfer (315), neural architecture (296), neural architecture search (221), network parameter (210), gpu day (180), neural architecture adaptation (174), partial standard transfer learning (160), preprint arxiv (150), neural network (130), architecture adaptation (125), standard transfer learning method (120), trinet era (110), general architecture (110), domain task (110), source domain (100), data mining (100), knowledge discovery (100), transfer learning method (95), target domain (90), object detection (90), full standard transfer learning (80), alaska meta transfer learning (80), neural architecture adaptation usa (80), computer vision (80), large scale dataset (79) |
| Abstract:             | The standard visual recognition systems follow the same pipeline: (1) pretraining a neural network on a large-scale dataset (e.g., ImageNet) and (2) finetuning the network parameters on a smaller, task-specific dataset. Such a pipeline for transfer learning hopes the sole parameter adaption is able to transfer the network capability from one domain to other domain, based on the strong assumption that a fixed network architecture is appropriate for all domain tasks. However, each domain task with distinct recognition target may need different levels / paths of feature hierarchy, where some neurons may become redundant and some others are re-activated to form new network structures. In this paper, we demonstrate that dynamically adapting network architectures tailored for each domain task along with parameter finetuning leads to more efficient and effective transfer learning methods, compared to the existing adaption pipeline which brute-forcedly tunes the parameters regardless of the network architecture bottleneck. We propose a new network architecture adaption pipeline, called as \emph{meta transfer learning}, to improve the generalization ability when applying the pretrained neural networks to new domains. Experimental results on a wide range of transfer learning tasks such as person re-identification, object detection, object segmentation, and image classification demonstrate not only performance gains but also interpretable neural architecture adaptation processes. |
| Submitted:            | Jan 14, 03:21 GMT                                            |
| Last update:          | Feb 04, 08:14 GMT                                            |
| Author conflicts:     | none                                                         |

| **Authors** |           |                            |         |                        |                                |                |
| ----------- | --------- | -------------------------- | ------- | ---------------------- | ------------------------------ | -------------- |
| first name  | last name | email                      | country | organization           | Web page                       | corresponding? |
| Guangrun    | Wang      | wanggrun@mail2.sysu.edu.cn | China   | Sun Yat-sen University | <https://wanggrun.github.io/>  | ✔              |
| Rongcong    | Chen      | chenrc@mail2.sysu.edu.cn   | China   | Sun Yat-sen University |                                |                |
| Guangcong   | Wang      | wanggc3@mail2.sysu.edu.cn  | China   | Sun Yat-sen University | <https://wanggcong.github.io/> |                |
| Jiefeng     | Peng      | jiefengpeng@gmail.com      | China   | Sun Yat-sen University |                                |                |
| Liang       | Lin       | linliang@ieee.org          | China   | Sun Yat-sen University | <http://www.linliang.net/>     |                |

### Reviews

| **Review 1**                                            |                                                              |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| *Three positive aspects of the paper:*                  | 1. This paper demonstrated that dynamically adapting network architectures tailored for each domain task along with parameter finetuning leads to more efficient and effective transfer learning methods. 2. Also, They proposed a new network architecture adaption pipeline, called as meta transfer learning, to improve the generalization ability when applying the pretrained neural networks to new domains. 3. Experimental results on various kinds of tasks validate the effectiveness of the proposed model. |
| *Three negative aspects of the paper:*                  | 1. Though there is an example of \beta architecture, how to exactly determine the real architecture of \beta? The authors modified (delete or reconnect layers) the architecture learnt from source domain to a new architecture for target domain. How about add a new layer? 2. Though the studied problem is important, the novelty of the proposed method is not significant and not impressed, which leads the limited contributions. 3. Rather than only evaluating the improved versions, how about the comparison with recent state-of-the-art transfer learning algorithms? For example, the work in [41]. |
| *Relevance to KDD Research track:*                      | **2**: (Very relevant: the paper is interesting to most researchers in the area) |
| *Overall evaluation:*                                   | Previous transfer learning algorithms assume that a fixed network architecture is appropriate for all domain tasks. However, each domain task with distinct recognition target may need different levels / paths of feature hierarchy, where some neurons may become redundant and some others are re-activated to form new network structures. Thus, the dynamic adaptation of model parameters and network architectures is very important. Overall, the writing is good, and I like it. Three main positive points are as follows, 1. This paper demonstrated that dynamically adapting network architectures tailored for each domain task along with parameter finetuning leads to more efficient and effective transfer learning methods. 2. Also, They proposed a new network architecture adaption pipeline, called as meta transfer learning, to improve the generalization ability when applying the pretrained neural networks to new domains. 3. Experimental results on various kinds of tasks validate the effectiveness of the proposed model. weak points: 1. Though there is an example of \beta architecture, how to exactly determine the real architecture of \beta? 2. The authors modified (delete or reconnect layers) the architecture learnt from source domain to a new architecture for target domain. How about add a new layer? 3. How to validate the interpretable? architecture changing processes? I think Fig.4 is not enough. 4. How to tune the numbers m and n? And how do they affect the performance? 5. There are m and n in Eq.6, however, in the "Learning and Optimization" Section, why in Eq. 8, they are summed only by m? 6. Also, in the experiments, the operations are chosen in a close set, how about in an open set? 7. The baselines in [27] and [40] are all from arXiv, and they are not from peer review journal or conference. Thus the comparison results maybe not appropriate. 8. Though the studied problem is important, the novelty of the proposed method is not significant and not impressed, which leads the limited contributions. 9. Rather than only evaluating the improved versions, how about the comparison with recent state-of-the-art transfer learning algorithms? For example, the work in [41]. |
| *Reproducibility:*                                      | **3**: (Yes - Fair. The information provided represents a fair effort to make it possible for readers to reproduce the results.) |
| *Please justify your answer regarding Reproducibility:* | Though the authors provide some details for the implementation, it is still not easy to reproduce the results for the ones who do not have enough computing resources. |
| *Overall Score:*                                        | **-1**: (Weak reject)                                        |

| **Review 2**                                            |                                                              |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| *Three positive aspects of the paper:*                  | 1.	Good performances on several benchmark datasets. Some of them are ne state of the art.  2.	Extensive experiments and comparisons presented evidences of the effectiveness of the algorithm in different application domains.  3.	The proposed algorithm is very efficient as compared with other automatic search from scratch methods. |
| *Three negative aspects of the paper:*                  | 1.	Description of the algorithm is not detailed enough, will have some difficulties to reproduce the work.  2.	The search spaces for the new architecture and weights are quite large, which will limit its applicability of the proposed method, because it requires a relatively large labeled dataset in the target domain.  3.	I would like to see whether the proposed method will work if target domain has a very limited labeled dataset. |
| *Relevance to KDD Research track:*                      | **2**: (Very relevant: the paper is interesting to most researchers in the area) |
| *Overall evaluation:*                                   | The paper presented a meta transfer learning method that started with a well trained standard computer vision model in source domain (usually ImageNet) and was finetuned to target domain by optimizing the model structure as well as its weights. In contrast to automatic search from scratch methods that are in terms of bottleneck, the meta transfer learning borrowed knowledge from source domain thus reduced the search space.   The proposed method was evaluated on couple different categories of benchmark datasets and achieved very good results in terms of both classification accuracy and computational efficiency. Some of the results have set new benchmark state of the arts. One limitation is that the search space is still quite large as compared to standard transfer learning because the model structure in target domain is also needed to be determined, which may require more labeled data in target domain for finetuning. Overall, the paper is very well written. |
| *Reproducibility:*                                      | **3**: (Yes - Fair. The information provided represents a fair effort to make it possible for readers to reproduce the results.) |
| *Please justify your answer regarding Reproducibility:* | Adequate information is provided to reproduce the work.      |
| *Overall Score:*                                        | **1**: (Weak Accept)                                         |

| **Review 3**                                            |                                                              |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| *Three positive aspects of the paper:*                  | 1. The neural architecture adaption problem is interesting and quite meaningful.  2. The manuscript is clearly written in general, and easy to follow. 3. Experiments are conducted on various tasks such as image classification and so forth. |
| *Three negative aspects of the paper:*                  | 1. The proposed model lacks detailed descriptions to certify that it is a meta-learning approach.  2. The methods or criteria for introducing the extra candidate architecture should be clarified.  3. Important results of baselines in terms of network depth and the number of model parameters are missing. |
| *Relevance to KDD Research track:*                      | **2**: (Very relevant: the paper is interesting to most researchers in the area) |
| *Overall evaluation:*                                   | In this work, the authors consider the problem of structure learning for neural networks by learning to transfer/meta-learning approach. In particular, network architectures and structures of the source and target domains are formulated in a learning problem. Experiments on multiple computer vision tasks are conducted including person re-identification, object detection & segmentation and image classification. However, there are several weaknesses in this work:   - The details of the formally defined objective function L in Eq. (10) is missing. Without the mathematical definition of L, it is fairly difficult to decide whether the proposed approach belongs to meta-learning/learning to transfer or not. Take the work DARTS (“DARTS: Differentiable Architecture Search”) as an example, its optimization problem in Eq. (5) is very clearly defined such that we know the architecture and parameter space are learnable under an optimizer-optimizee learning-like procedure (“Learning to learn by gradient descent by gradient descent”).   - The rationale behind the extra architecture beta is also unclear. While it seems intuitive a domain-specific architecture should be considered in modeling. The introduction of beta is quite arbitrary. What is the relation between the architecture space and the optimization procedure? The formal optimization with respect to all the parameters and architecture variables should be shown in equations or a pseudo algorithm.   - Experimental results should show not only the depth of neural networks but also the number of learned parameters. Thus, we can judge the complexity of the automatically learned architectures. In addition, the depth of baselines is missing in the comparison of CIFAR-10. NAS (“neural architecture search with reinforcement learning”) has provided its depth as 39.   - To show the effectiveness of the proposed method, learning curves of the neural architecture search under the given experimental settings are highly recommended to be presented. On the other hand, the case study of several learned cell structures is also encouraged to be given.   Suggestion 1. As a research track paper, the contribution of this work should be claimed from the perspective of the model novelty.  2. More implementation details including source code are highly recommended to be shared.  3. Not only the transferred layer-level structures but also the learned cell structures are helpful to demonstrate the effectiveness of the proposed method. |
| *Reproducibility:*                                      | **2**: (Yes - Poor. Some description provided, but it is clearly insufficient information for reproducibility.) |
| *Please justify your answer regarding Reproducibility:* | It is rather difficult to implement this work. Since the objective function and optimization details are skipped, meanwhile the source codes are not provided. |
| *Overall Score:*                                        | **-1**: (Weak reject)                                        |

| **Review 4**                                            |                                                              |
| ------------------------------------------------------- | ------------------------------------------------------------ |
| *Three positive aspects of the paper:*                  | 1) The problem the paper trying to solve is important. 2) The experimental results show some promising results. |
| *Three negative aspects of the paper:*                  | 1) The problem setting is not new. Since in neural architecture search, there are some studies on using the parameter discovered to confine the search space, for example, "Efficient Neural Architecture Search via Parameter Sharing", so it is already jointly optimizing the architecture and the parameters. 2) The problem formulation is very general but the approach just explore a little bit. In equation (4), the formulation is too general that any models could fit into. However, the candidates are focusing on convolution based architectures. 3) The major part which is how the beta-artchitecture is introduced is not clear from section 3.4. Besides, a running example about how the architectures are introduced and evolved could be helpful as well. |
| *Relevance to KDD Research track:*                      | **2**: (Very relevant: the paper is interesting to most researchers in the area) |
| *Overall evaluation:*                                   | The paper studies the problem of jointly optimizing neural network parameters and structures for the purpose of better transfer learning. The idea is in general sound. However, the experimental results lack comparison to some important related approaches.  The strengths of the paper: 1) The problem the paper trying to solve is important. 2) The experimental results show some promising results.  The weakness of the paper: 1) The problem setting is not new. Since in neural architecture search, there are some studies on using the parameter discovered to confine the search space, for example, "Efficient Neural Architecture Search via Parameter Sharing", so it is already jointly optimizing the architecture and the parameters. 2) The problem formulation is very general but the approach just explore a little bit. In equation (4), the formulation is too general that any models could fit into. However, the candidates are focusing on convolution based architectures. 3) The major part which is how the beta-artchitecture is introduced is not clear from section 3.4. Besides, a running example about how the architectures are introduced and evolved could be helpful as well. 4) It is worth comparing to many architecture search studies both from results and performance point of view. For example, "Learning Transferable Architectures for Scalable Image Recognition", "Efficient Neural Architecture Search via Parameter Sharing", "SIMPLE AND EFFICIENT ARCHITECTURE SEARCH FOR CONVOLUTIONAL NEURAL NETWORKS". 5) Some results on text, such as NLP tasks could make the experiments more convincing. |
| *Reproducibility:*                                      | **3**: (Yes - Fair. The information provided represents a fair effort to make it possible for readers to reproduce the results.) |
| *Please justify your answer regarding Reproducibility:* | The paper studies the problem of jointly optimizing neural network parameters and structures for the purpose of better transfer learning. The idea is in general sound. However, the experimental results lack comparison to some important related approaches. |
| *Overall Score:*                                        | **-1**: (Weak reject)                                        |

### Metareview

| Metareview for paper 31 |                                                              |
| ----------------------- | ------------------------------------------------------------ |
| Title:                  | Meta Transfer Learning by Neural Architecture Adaptation     |
| Authors:                | Guangrun Wang, Rongcong Chen, Guangcong Wang, Jiefeng Peng and Liang Lin |
| Recommendation:         | **reject**                                                   |
| Text:                   | The reviewers have strong concerns on motivation, technical details and novelty of the proposed solutions. Moreover, the experimental study is also weak, not comparing with state of art approaches. |